import os
import re
import time
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chromadb import PersistentClient

# ===============================================
# í™˜ê²½ ì„¤ì •
# ===============================================
load_dotenv()
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

genai.configure(api_key=api_key)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TXT_DIR = os.path.join(BASE_DIR, "new2_texts")
CHROMA_DB_PATH = os.path.join(BASE_DIR, "chroma_db")
COLLECTION_NAME = "txt_collection"

EMBEDDING_MODEL = "models/text-embedding-004"

# ===============================================
# 1. í•µì‹¬ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ (Metadata Parser)
# ===============================================
def extract_core_info(text):
    """
    ê°•ì˜ê³„íšì„œ í…ìŠ¤íŠ¸ì—ì„œ ë‹´ë‹¹êµìˆ˜, í•™ë…„, í•™ì  ë“±ì„ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    info = {
        "professor": "ì •ë³´ì—†ìŒ",
        "grade": "ì •ë³´ì—†ìŒ",
        "credit": "ì •ë³´ì—†ìŒ"
    }
    
    # 1. ë‹´ë‹¹êµìˆ˜ ì¶”ì¶œ (ì˜ˆ: * **ë‹´ë‹¹êµìˆ˜:** ì˜¤ë¯¼ì‹)
    prof_match = re.search(r"\*\*ë‹´ë‹¹êµìˆ˜:\*\*\s*([^\n]+)", text)
    if prof_match:
        info["professor"] = prof_match.group(1).strip()

    # 2. ëŒ€ìƒí•™ë…„ ì¶”ì¶œ (ì˜ˆ: * **ëŒ€ìƒí•™ë…„:** 3í•™ë…„)
    grade_match = re.search(r"\*\*ëŒ€ìƒí•™ë…„:\*\*\s*([^\n]+)", text)
    if grade_match:
        info["grade"] = grade_match.group(1).strip()

    # 3. í•™ì  ì¶”ì¶œ
    credit_match = re.search(r"\*\*í•™ì /ì‹œê°„:\*\*\s*([^\n]+)", text)
    if credit_match:
        info["credit"] = credit_match.group(1).strip()
        
    return info

# ===============================================
# 2. íŒŒì¼ ë¡œë“œ ë° ì²­í¬ ìƒì„± (ì •ë³´ ì£¼ì…)
# ===============================================
def load_and_chunk_files(txt_files):
    all_chunks = []
    all_metadatas = []

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )

    print(f"--- íŒŒì¼ ì²˜ë¦¬ ë° ì •ë³´ ì¶”ì¶œ ì‹œì‘ ({len(txt_files)}ê°œ) ---")

    for path in txt_files:
        try:
            file_name = os.path.splitext(os.path.basename(path))[0]
            
            with open(path, "r", encoding="utf-8") as f:
                raw_text = f.read()

            # ğŸ”¥ í•µì‹¬: íŒŒì¼ì—ì„œ ì¤‘ìš” ì •ë³´ ë¯¸ë¦¬ ë½‘ê¸°
            info = extract_core_info(raw_text)
            
            # ì²­í¬ ìƒì„±
            chunks = splitter.split_text(raw_text)

            # ğŸ”¥ í•µì‹¬: ëª¨ë“  ì²­í¬ì— ì •ë³´ ì£¼ì… (í—¤ë” ì—…ê·¸ë ˆì´ë“œ)
            for chunk in chunks:
                # ì˜ˆ: [ê°•ì˜ëª…: í†µê³„ì ë°ì´í„°ë¶„ì„ | êµìˆ˜: ì˜¤ë¯¼ì‹ | í•™ë…„: 3í•™ë…„]
                header_tag = f"[ê°•ì˜ëª…: {file_name} | êµìˆ˜: {info['professor']} | í•™ë…„: {info['grade']}]"
                enhanced_chunk = f"{header_tag}\n{chunk}"
                
                all_chunks.append(enhanced_chunk)
                
                # ë©”íƒ€ë°ì´í„°ì—ë„ ì €ì¥ (ë‚˜ì¤‘ì— í•„í„°ë§ ê°€ëŠ¥í•˜ë„ë¡)
                all_metadatas.append({
                    "source": file_name,
                    "professor": info['professor'],
                    "grade": info['grade']
                })

            print(f"âœ… {file_name} -> êµìˆ˜: {info['professor']}")

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì˜¤ë¥˜ {path}: {e}")

    return all_chunks, all_metadatas

# ===============================================
# 3. ì„ë² ë”© ìƒì„±
# ===============================================
def get_embeddings_for_chunks(chunks):
    embeddings = []
    total = len(chunks)
    batch_size = 10 

    print(f"\n--- ì„ë² ë”© ìƒì„± ì‹œì‘ (ì´ {total}ê°œ) ---")

    for i in range(0, total, batch_size):
        batch = chunks[i : i + batch_size]
        try:
            result = genai.embed_content(
                model=EMBEDDING_MODEL,
                content=batch,
                task_type="retrieval_document",
            )
            if 'embedding' in result:
                embeddings.extend(result['embedding'])
            
            print(f"  â†’ {min(i + batch_size, total)}/{total} ì²˜ë¦¬ ì™„ë£Œ", end="\r")
            time.sleep(1)  
        except Exception as e:
            print(f"\nâŒ ë°°ì¹˜ ì˜¤ë¥˜: {e}")
            continue

    print("\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    return embeddings

# ===============================================
# 4. ChromaDB ì €ì¥
# ===============================================
def build_rag_database():
    if not os.path.exists(TXT_DIR):
        os.makedirs(TXT_DIR)
        print(f"ğŸ“ '{TXT_DIR}' í´ë”ê°€ ì—†ì–´ì„œ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        return

    txt_files = [
        os.path.join(TXT_DIR, f) for f in os.listdir(TXT_DIR) if f.lower().endswith(".txt")
    ]

    if not txt_files:
        print(f"âŒ '{TXT_DIR}' í´ë”ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. ë¡œë“œ ë° ì •ë³´ ì£¼ì…
    chunks, metadatas = load_and_chunk_files(txt_files)
    
    # 2. ì„ë² ë”©
    embeddings = get_embeddings_for_chunks(chunks)

    if len(embeddings) != len(chunks):
        print("âŒ ì„ë² ë”© ê°œìˆ˜ ì˜¤ë¥˜")
        return

    # 3. ì €ì¥
    print("\n--- ChromaDB ì €ì¥ ì¤‘ ---")
    try:
        client = PersistentClient(path=CHROMA_DB_PATH)
        existing = [c.name for c in client.list_collections()]
        if COLLECTION_NAME in existing:
            client.delete_collection(COLLECTION_NAME)
            print("ğŸ—‘  ê¸°ì¡´ DB ì‚­ì œë¨")

        collection = client.get_or_create_collection(COLLECTION_NAME)
        
        ids = [f"doc_{i}" for i in range(len(chunks))]

        collection.add(
            documents=chunks,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids,
        )
        print(f"âœ… ì €ì¥ ì™„ë£Œ (ì´ {collection.count()}ê°œ)")

    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    build_rag_database()